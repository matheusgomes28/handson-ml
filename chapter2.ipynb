{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas to Parse CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH, housing_name=\"housing.csv\"):\n",
    "    csv_path = os.path.join(housing_path, housing_name)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data Description with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make sure we have a reproducible code\n",
    "np.random.seed(42)\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(\"We now have {:d} train samples and {:d} test samples.\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hashes to Maintain a Consistent Dataset\n",
    "\n",
    "If you want to keep a consistent test and train set, then even the `np.random.seed(n)` will not work as the indices will be moved around if the dataset is updated. One way to maitain a consistent track of the test/train instances is to use their hashes to decide whether or not they will be included in a set. Can use the last `N` bytes to decide the threashold:\n",
    "\n",
    "\n",
    " - Last two bytes can go up to 255: Check would be  $ d(XX) < 255 * \\lambda $\n",
    " - Last four bytes can go up to 65535: Check would be  $ d(XXXX) < 65535 * \\lambda $\n",
    " \n",
    " where $ d(X...) $ converts digits to decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def test_set_check_last2(identifier, test_ratio, hash_f):\n",
    "    return hash_f(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
    "\n",
    "def test_set_check_last4(identifier, test_ratio, hash_f):\n",
    "    bites = hash_f(np.int64(identifier)).digest()[-2:]\n",
    "    return int.from_bytes(bites, \"little\") < 65535 * test_ratio\n",
    "\n",
    "def split_set_by_id(data, test_ratio, id_column, hash_f=hashlib.md5):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check_last2(id_, test_ratio, hash_f))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas-Generated Index as ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index() # Adds index column\n",
    "train_set, test_set = split_set_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Longitude and Latitude for ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_set_by_id(housing_with_id, 0.2, \"id\")\n",
    "\n",
    "# Check if we have none in intersection.\n",
    "train_set.merge(test_set, on=[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn to Split\n",
    "\n",
    "Scikit-Learn offers functions to split test/train sets in the same way as we implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This will randomly select 20% of the data as test data\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Strata Categories \n",
    "\n",
    "Dividing the `median_category` income to limit the number of strata (groups), and also cap the income category to value `5` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "train_index, test_index = next(split.split(housing, housing[\"income_cat\"]))\n",
    "strat_train_set = housing.loc[train_index]\n",
    "strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"income_cat\" field as it's no longer needed\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Train Data by by Copying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()\n",
    "\n",
    "# Clearly plotting the geographical location along\n",
    "# with other information such as area population\n",
    "housing.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    alpha=0.4,\n",
    "    s=housing[\"population\"]/100,\n",
    "    label=\"population\",\n",
    "    c=\"median_house_value\",\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True,\n",
    "    figsize=(20,12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Correlation Matrix\n",
    "\n",
    "correlation = housing.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(20,12))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Attribute Combination\n",
    "\n",
    "Sometimes combining attributes is useful for feeding into machine learning algorithms. For example, combining the \n",
    "total number of rooms with the number of households will provide the average number of bedrooms per household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"roms_per_household\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"beds_per_household\"] = housing[\"total_bedrooms\"] / housing[\"households\"]\n",
    "housing[\"population_per_household\"] = housing[\"population\"] / housing[\"households\"]\n",
    "\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up and Applying Transformation to Data\n",
    "\n",
    "Certain data transformations may be helpful on dataset. For example, some machine learning algorithms don't allow empty fields, or we may want to make the distribution of a feature more normally distributed.\n",
    "\n",
    "We will first start by making a fresh copy of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1).copy()\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning Up Missing Fields with Vanilla Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only the indices (rows) with missing values in the\n",
    "# subset provided\n",
    "drop_missing_rows = housing.dropna(subset=[\"total_bedrooms\"], inplace=False)\n",
    "\n",
    "# Drop the entire feature column for a feature that has\n",
    "# missing entries\n",
    "drop_missing_column = housing.drop(\"total_bedrooms\", axis=1, inplace=False)\n",
    "\n",
    "# Fill the missing entries with the median value\n",
    "# which you calculate\n",
    "median = housing[\"total_bedrooms\"].median() # Save for later use in missing tests\n",
    "fill_missing_rows = housing[\"total_bedrooms\"].fillna(median, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Missing Fields with Scikit's Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# SimpleImputer replaced Imputer since the book has\n",
    "# been released\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# The imputer can only work with numerical data as it\n",
    "# calculated the median for each column/row\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1, inplace=False)\n",
    "\n",
    "# It computed the median for each attribute and saves them in\n",
    "# it's statistics_ field\n",
    "imputer.fit(housing_num)\n",
    "imputer.statistics_\n",
    "\n",
    "# To apply the transformation (add the median values),\n",
    "# we must transform the data which creates a numpy array.\n",
    "# We then put it back into a pandas DataFrame\n",
    "transformed_matrix = imputer.transform(housing_num)\n",
    "# Could also have done it with a single call to\n",
    "# transformed_matrix = imputer.fit_transform(housing_num)\n",
    "housing_tr = pd.DataFrame(transformed_matrix, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Text Attributes to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to encode the text into integers\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded, category_indices = housing_cat.factorize()\n",
    "print(housing_cat_encoded[:10])\n",
    "print(category_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Text Attributes into Sparse Binary Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(dtype=np.int16)\n",
    "housing_cat_onehot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_onehot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Tranformer Classes\n",
    "\n",
    "For a transformer, we need a class that has the follwoing methods:\n",
    "\n",
    "* `fit()`, a a method returning `self`\n",
    "* `transform()`\n",
    "* `fit_transform()`\n",
    "\n",
    "If superclass is `TransformerMixin`, we get `fit_transform()` for free. If superclass is `BaseEstimator` we get `get_params()` and `set_params()` which are useful for automatic hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributeAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Note: No *args or **kwargs because of the base\n",
    "    # estimator subclass (for some reason)\n",
    "    def __init__(self, add_bedrooms_per_room = True,\n",
    "                 rooms_i = rooms_ix,\n",
    "                 bedrooms_i = bedrooms_ix,\n",
    "                 population_i = population_ix,\n",
    "                 household_i = household_ix):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        self.rooms_i = rooms_i\n",
    "        self.bedrooms_i = bedrooms_i\n",
    "        self.population_i = population_i\n",
    "        self.household_i = household_i\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, self.rooms_i] / X[:, self.household_i]\n",
    "        population_per_household = X[:, self.population_i] / X[:, self.household_i]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, self.bedrooms_i] / X[:, self.rooms_i]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributeAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Transformers with Pipelines\n",
    "\n",
    "Scikit-learn provides the class `Pipeline` which simply combines transformers implementing `fit()` and `transform()`. It will apply them in the order that you provide, and apparently, the last transformers only need to implement `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"attrib_adder\", CombinedAttributeAdder(add_bedrooms_per_room=True)),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline That Takes a Pandas Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a class that takes pandas and converts to numpy\n",
    "class AlphabeticalDataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.attribute_names.sort()        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].to_numpy()\n",
    "    \n",
    "num_columns = list(housing_num.columns)\n",
    "cat_columns  = [\"ocean_proximity\"]\n",
    "categories = sorted(list(housing[\"ocean_proximity\"].unique()))\n",
    "\n",
    "# Pipeline to process the numerical values\n",
    "num_pipeline = Pipeline([\n",
    "    (\"selector\", AlphabeticalDataFrameSelector(num_columns)),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"attrib_adder\", CombinedAttributeAdder()),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline to process the categorical values\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"selector\", AlphabeticalDataFrameSelector(cat_columns)),\n",
    "    (\"cat_encoder\", OneHotEncoder(categories=[categories],dtype=np.int16))\n",
    "])\n",
    "\n",
    "# In the OneHotEncoder transform, we must provide the sorted list of expected\n",
    "# categories (for each feature, hence a nested list) so the pipeline \n",
    "# ***always*** generates the expected size for the one-hot-encoding sparse\n",
    "# matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining/Concatenating Pipeline Results\n",
    "\n",
    "Scikit-learn provides the `FeatureUnion` class which runs all the individual pipelines/transformer provided in parallel and then concatenates the result of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_piepline\", cat_pipeline)\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialising Models\n",
    "\n",
    "As we can see, some of these models take quite a long time to train (RandomDecisionRegressor), \n",
    "so saving them for future uses would be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path\n",
    "from joblib import dump, load\n",
    "\n",
    "def dump_model(model, model_name):\n",
    "    \n",
    "    # Create model folder if necessary\n",
    "    if not os.path.exists(\"models\") or not os.path.isdir(\"models\"):\n",
    "        print(\"Making dir\")\n",
    "        os.mkdir(\"models\")\n",
    "        \n",
    "    # dump the file if it already does not exist\n",
    "    path = \"models/{:s}\".format(model_name)\n",
    "    path = os.path.join(\"models\", model_name)\n",
    "    if not os.path.exists(path) and not os.path.isdir(path):\n",
    "        dump(model, path)\n",
    "        \n",
    "def load_model(model_name):\n",
    "    \n",
    "    # Only load if valid\n",
    "    path = os.path.join(\"models\", model_name)\n",
    "    if os.path.exists(path) and not os.path.isdir(path):\n",
    "        return load(path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model - Linear Regression\n",
    "\n",
    "Training is easy with scikit-learn. LinearRegression is an estimator, providing a `fit()` function and `predict()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = load_model(\"lin_reg.pkl\")\n",
    "if not lin_reg:\n",
    "    print(\"Training the model...\")\n",
    "    # Training/fitting the model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Testing / predicting with the model\n",
    "some_data = housing.iloc[:10]\n",
    "some_labels = housing_labels[:5]\n",
    "some_data_prepared = full_pipeline.fit_transform(some_data)\n",
    "\n",
    "for pred, actual in zip(lin_reg.predict(some_data_prepared), list(some_labels)):\n",
    "    print(\"Predicted {:10.2f};  Actual: {:10.2f}\".format(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model - Decision Tree Regressor\n",
    "\n",
    "As we can see in mean square error above, each prediction has an average of £69,000 error, which does not make it great as a price predictor. Let's try a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = load_model(\"tree_reg.pkl\")\n",
    "if not tree_reg:\n",
    "    print(\"Training model\")\n",
    "    # Fit and predict the labels\n",
    "    tree_reg = DecisionTreeRegressor()\n",
    "    tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Models with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_tree = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\",\n",
    "                              cv=10, n_jobs=4)\n",
    "tree_rmse_scores = np.sqrt(-scores_tree)\n",
    "\n",
    "scores_linear = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\",\n",
    "                                cv=10, n_jobs=4)\n",
    "linear_rmse_scores = np.sqrt(-scores_linear)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Sandard deviation:\", scores.std())\n",
    "    \n",
    "print(\"SCORES FOR DECISION TREE\")\n",
    "display_scores(tree_rmse_scores)\n",
    "print()\n",
    "print(\"SCORES FOR LINEAR REGRESSION\")\n",
    "display_scores(linear_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = load_model(\"forest_reg.pkl\")\n",
    "if not forest_reg:\n",
    "    forest_reg = RandomForestRegressor()\n",
    "    forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "scores_random = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10, n_jobs=4)\n",
    "random_rmse_scores = np.sqrt(-scores_random)\n",
    "\n",
    "print(\"SCORES FOR RANDOM FOREST\")\n",
    "display_scores(random_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Model Parameters with Grid Search\n",
    "\n",
    "Scikit-learn's `GridSearchCV` seems like a brute-force estimator which tries all combinations of hypter-parameters you give. Pick the best parameters by analysing the cross-validation performance.\n",
    "\n",
    "For bigger searches, see `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\"n_estimators\": [3, 10, 30], \"max_features\": [2, 3, 6, 8]},\n",
    "    {\"bootstrap\": [False], \"n_estimators\": [3, 10], \"max_features\": [2, 3, 4]}\n",
    "]\n",
    "\n",
    "forest_grid_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_grid_reg, param_grid, cv=5,\n",
    "                           scoring=\"neg_mean_squared_error\", n_jobs=4)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator is saved in best_estimator_\n",
    "print(\"Best estimator params are:\", grid_search.best_params_)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Print the performance of all the cross-validations\n",
    "cv_results = grid_search.cv_results_\n",
    "for neg_mean, params in zip (cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
    "    mean = np.sqrt(-neg_mean)\n",
    "    print(\"Mean error is: {:10.2f}; Params are {:s}\".format(mean, str(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Best Fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "\n",
    "# we can get the transformer from a pipeline\n",
    "# with this\n",
    "cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "cat_onehot_attribs = list(cat_encoder.categories_[0])\n",
    "\n",
    "attributes = sorted(num_columns) + extra_attribs + cat_onehot_attribs\n",
    "importance = best_estimator.feature_importances_\n",
    "importance_attributes = sorted(zip(attributes, importance), key=lambda x: x[1], reverse=True) \n",
    "for attrib, importance in importance_attributes:\n",
    "    print(\"Feature Importance of {:18s} is {:7.4f}\".format(attrib, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The cells below will implement each of the exerises for chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Using a SVM Regressor as the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_object = SVR()\n",
    "svr_grid_params = [\n",
    "    {\"kernel\": [\"linear\"], \"C\": [0.1, 0.5, 1.0, 3.0], \"gamma\": [\"scale\", \"auto\"]},\n",
    "    {\"kernel\": [\"rbf\"], \"C\": [0.1, 0.7, 1.5, 3.0, 4.0], \"gamma\": [\"scale\", \"auto\"]},\n",
    "    {\"kernel\": [\"poly\"], \"degree\": [2, 3, 4, 5], \"C\": [0.5, 0.7, 3.0, 5.0, 7.0], \"gamma\": [\"auto\"]}\n",
    "]\n",
    "\n",
    "svr_grid = GridSearchCV(svr_object, svr_grid_params, cv=5,\n",
    "                           scoring=\"neg_mean_squared_error\", n_jobs=4)\n",
    "svr_grid.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "svr_cv_results = svr_grid.cv_results_\n",
    "for neg_mean, params in zip(svr_cv_results[\"mean_test_score\"], svr_cv_results[\"params\"]):\n",
    "    mean = np.sqrt(-neg_mean)\n",
    "    print(\"Mean error is: {:10.2f}; Params are {:s}\".format(mean, str(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Using the RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "svr_object = SVR()\n",
    "svr_dist = dict(C=uniform(loc=0.1, scale=5))\n",
    "svr_rand_search = RandomizedSearchCV(svr_object, svr_dist, n_jobs=4, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "svr_rand_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "svr_rand_results = svr_rand_search.cv_results_\n",
    "for neg_mean, params in zip(svr_rand_results[\"mean_test_score\"], svr_rand_results[\"params\"]):\n",
    "    mean = np.sqrt(-neg_mean)\n",
    "    print(\"Mean error is: {:10.2f}; Params are {:s}\".format(mean, str(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Adding Best Feature Transformer to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "class BestFeatureSelector(BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.features].to_numpy()\n",
    "    \n",
    "best_features = [\"median_income\",\n",
    "                \"bedrooms_per_room\",\n",
    "                \"longitude\",\n",
    "                \"latitude\",\n",
    "                \"pop_per_hhold\"]\n",
    "\n",
    "\n",
    "starting_features = [\"total_rooms\",\n",
    "                     \"total_bedrooms\",\n",
    "                     \"population\",\n",
    "                     \"households\",\n",
    "                     \"median_income\"]\n",
    "                     \n",
    "# 1st stage = housing[starting_features]\n",
    "# 2nd stage = housing[starting_features + [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]]\n",
    "ex_num_pipeline = make_pipeline(BestFeatureSelector(starting_features),\n",
    "                                SimpleImputer(strategy=\"median\"),\n",
    "                                CombinedAttributeAdder(rooms_i=0, bedrooms_i=1, population_i=2, household_i=3),\n",
    "                                StandardScaler())\n",
    "\n",
    "ex_cat_pipeline = make_pipeline(BestFeatureSelector([\"ocean_proximity\"]),\n",
    "                               OneHotEncoder(categories=[categories],dtype=np.int16))\n",
    "\n",
    "ex_union_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", ex_num_pipeline),\n",
    "    (\"cat_piepline\", ex_cat_pipeline)\n",
    "])\n",
    "\n",
    "ex_housing_prepared = ex_union_pipeline.fit_transform(housing, housing_labels)[:, [4, 5, 6, 7, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svr_object = SVR()\n",
    "svr_grid_params = [\n",
    "    {\"kernel\": [\"linear\"], \"C\": [0.1, 0.5, 1.0, 3.0], \"gamma\": [\"scale\", \"auto\"]},\n",
    "    {\"kernel\": [\"rbf\"], \"C\": [0.1, 0.7, 1.5, 3.0, 4.0], \"gamma\": [\"scale\", \"auto\"]},\n",
    "    {\"kernel\": [\"poly\"], \"degree\": [2, 3, 4, 5], \"C\": [0.5, 0.7, 3.0, 5.0, 7.0], \"gamma\": [\"auto\"]}\n",
    "]\n",
    "\n",
    "svr_grid = GridSearchCV(svr_object, svr_grid_params, cv=5,\n",
    "                           scoring=\"neg_mean_squared_error\", n_jobs=4)\n",
    "svr_grid.fit(ex_housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "svr_cv_results = svr_grid.cv_results_\n",
    "for neg_mean, params in zip(svr_cv_results[\"mean_test_score\"], svr_cv_results[\"params\"]):\n",
    "    mean = np.sqrt(-neg_mean)\n",
    "    print(\"Mean error is: {:10.2f}; Params are {:s}\".format(mean, str(params)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
